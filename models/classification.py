# -*- coding: utf-8 -*-
"""
Klasa do klasyfikacji u≈ºywania substancji UCI Drug Consumption
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from typing import Dict, List, Optional, Tuple
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix
from sklearn.preprocessing import StandardScaler

from utils.constants import PERSONALITY_COLS, SUBSTANCE_COLS, COLORS
from utils.helpers import get_substance_category, get_performance_status

class ClassificationManager:
    """Klasa do zarzƒÖdzania modelami klasyfikacji"""

    def __init__(self):
        self.scaler = StandardScaler()
        self.models = {}
        self.results_cache = {}

    def perform_classification(self, df: pd.DataFrame, substance: str) -> Dict:
        """
        Przeprowadza klasyfikacjƒô dla danej substancji
        
        Args:
            df: DataFrame z danymi
            substance: Nazwa substancji do przewidywania
            
        Returns:
            S≈Çownik z wynikami analizy
        """
        try:
            # Sprawdzenie danych
            if substance not in df.columns:
                return {
                    'success': False,
                    'message': f"Brak danych dla substancji: {substance}",
                    'analysis_text': '',
                    'figure': None
                }

            # Przygotowanie danych
            existing_personality_cols = [col for col in PERSONALITY_COLS if col in df.columns]

            if len(existing_personality_cols) < 3:
                return {
                    'success': False,
                    'message': "Za ma≈Ço cech osobowo≈õci do klasyfikacji",
                    'analysis_text': '',
                    'figure': None
                }

            features = df[existing_personality_cols].dropna()
            target = (df.loc[features.index, substance] > 0).astype(int)

            # Sprawdzenie rozk≈Çadu klas
            usage_count = target.sum()
            non_usage_count = len(target) - usage_count

            if usage_count < 10 or non_usage_count < 10:
                return {
                    'success': False,
                    'message': f"Za ma≈Ço danych dla {substance}. Potrzeba minimum 10 os√≥b w ka≈ºdej grupie.",
                    'analysis_text': '',
                    'figure': None
                }

            # Podzia≈Ç danych
            X_train, X_test, y_train, y_test = train_test_split(
                features, target, test_size=0.3, random_state=42, stratify=target
            )

            # Trenowanie modelu
            rf = RandomForestClassifier(n_estimators=100, random_state=42)
            rf.fit(X_train, y_train)

            # Predykcja
            y_pred = rf.predict(X_test)
            y_pred_proba = rf.predict_proba(X_test)[:, 1]

            # Metryki
            accuracy = accuracy_score(y_test, y_pred)
            baseline = max(target.mean(), 1 - target.mean())
            improvement = accuracy - baseline

            try:
                auc_score = roc_auc_score(y_test, y_pred_proba)
            except:
                auc_score = 0.5

            conf_matrix = confusion_matrix(y_test, y_pred)

            # Wa≈ºno≈õƒá cech
            feature_importance = pd.DataFrame({
                'Feature': existing_personality_cols,
                'Importance': rf.feature_importances_
            }).sort_values('Importance', ascending=False)

            # Przygotowanie wynik√≥w
            analysis_text = self._generate_classification_report(
                substance, features, target, accuracy, baseline, improvement,
                auc_score, conf_matrix, feature_importance, X_train, X_test
            )

            figure = self._create_classification_plots(
                feature_importance, accuracy, baseline, substance, auc_score
            )

            # Cache wynik√≥w
            self.results_cache[substance] = {
                'accuracy': accuracy,
                'baseline': baseline,
                'improvement': improvement,
                'auc': auc_score,
                'feature_importance': feature_importance,
                'usage_rate': target.mean()
            }

            return {
                'success': True,
                'message': 'Klasyfikacja zako≈Ñczona pomy≈õlnie',
                'analysis_text': analysis_text,
                'figure': figure
            }

        except Exception as e:
            return {
                'success': False,
                'message': f"B≈ÇƒÖd klasyfikacji: {str(e)}",
                'analysis_text': '',
                'figure': None
            }

    def compare_all_substances(self, df: pd.DataFrame) -> Dict:
        """
        Por√≥wnuje klasyfikacje wszystkich substancji
        
        Args:
            df: DataFrame z danymi
            
        Returns:
            S≈Çownik z wynikami por√≥wnania
        """
        results = []
        existing_personality_cols = [col for col in PERSONALITY_COLS if col in df.columns]
        existing_substance_cols = [col for col in SUBSTANCE_COLS if col in df.columns]

        if len(existing_personality_cols) < 3:
            return {
                'analysis_text': "Za ma≈Ço cech osobowo≈õci do analizy klasyfikacji",
                'figure': None
            }

        features = df[existing_personality_cols].dropna()

        for substance in existing_substance_cols:
            target = (df.loc[features.index, substance] > 0).astype(int)
            usage_count = target.sum()
            non_usage_count = len(target) - usage_count

            if usage_count < 20 or non_usage_count < 20:
                results.append({
                    'Substance': substance,
                    'Users': usage_count,
                    'Non_Users': non_usage_count,
                    'Usage_Rate': target.mean(),
                    'Accuracy': np.nan,
                    'AUC': np.nan,
                    'Baseline': np.nan,
                    'Improvement': np.nan,
                    'Top_Feature': 'Insufficient data',
                    'Status': 'Insufficient data',
                    'Category': get_substance_category(substance)
                })
                continue

            try:
                X_train, X_test, y_train, y_test = train_test_split(
                    features, target, test_size=0.3, random_state=42, stratify=target
                )

                rf = RandomForestClassifier(n_estimators=100, random_state=42)
                rf.fit(X_train, y_train)

                y_pred = rf.predict(X_test)
                y_pred_proba = rf.predict_proba(X_test)[:, 1]

                accuracy = accuracy_score(y_test, y_pred)
                baseline = max(target.mean(), 1 - target.mean())
                improvement = accuracy - baseline

                try:
                    auc_score = roc_auc_score(y_test, y_pred_proba)
                except:
                    auc_score = 0.5

                feature_importance = pd.DataFrame({
                    'Feature': existing_personality_cols,
                    'Importance': rf.feature_importances_
                }).sort_values('Importance', ascending=False)

                top_feature = feature_importance.iloc[0]['Feature']
                status = get_performance_status(accuracy, baseline, auc_score)

                results.append({
                    'Substance': substance,
                    'Users': usage_count,
                    'Non_Users': non_usage_count,
                    'Usage_Rate': target.mean(),
                    'Accuracy': accuracy,
                    'AUC': auc_score,
                    'Baseline': baseline,
                    'Improvement': improvement,
                    'Top_Feature': top_feature,
                    'Status': status,
                    'Category': get_substance_category(substance)
                })

            except Exception as e:
                results.append({
                    'Substance': substance,
                    'Users': usage_count,
                    'Non_Users': non_usage_count,
                    'Usage_Rate': target.mean(),
                    'Accuracy': np.nan,
                    'AUC': np.nan,
                    'Baseline': np.nan,
                    'Improvement': np.nan,
                    'Top_Feature': f'Error: {str(e)[:20]}',
                    'Status': '‚ùå Error',
                    'Category': get_substance_category(substance)
                })

        results_df = pd.DataFrame(results)

        # POPRAWKA: U≈ºyj dropna() zamiast na_last (kompatybilno≈õƒá z starszymi wersjami pandas)
        # Najpierw usu≈Ñ wiersze z NaN w kolumnie Improvement
        results_df_clean = results_df.dropna(subset=['Improvement'])
        results_df_nan = results_df[results_df['Improvement'].isna()]

        # Sortuj czyste dane
        results_df_clean = results_df_clean.sort_values('Improvement', ascending=False)

        # Po≈ÇƒÖcz z powrotem (NaN na ko≈Ñcu)
        results_df = pd.concat([results_df_clean, results_df_nan], ignore_index=True)

        analysis_text = self._generate_comparison_report(results_df)
        figure = self._create_comparison_plots(results_df)

        return {
            'analysis_text': analysis_text,
            'figure': figure
        }

    def _generate_classification_report(self, substance: str, features: pd.DataFrame,
                                        target: pd.Series, accuracy: float, baseline: float,
                                        improvement: float, auc_score: float,
                                        conf_matrix: np.ndarray, feature_importance: pd.DataFrame,
                                        X_train: pd.DataFrame, X_test: pd.DataFrame) -> str:
        """Generuje raport klasyfikacji dla pojedynczej substancji"""

        text = f"""
üå≤ === KLASYFIKACJA RANDOM FOREST - {substance.upper()} ===

üéØ ZADANIE: Przewidywanie u≈ºywania {substance} na podstawie cech osobowo≈õci

üìä PODSTAWOWE INFORMACJE:
‚îú‚îÄ‚îÄ Ca≈Çkowita pr√≥ba: {len(features)} os√≥b
‚îú‚îÄ‚îÄ üü¢ U≈ºywa substancji: {target.sum()} os√≥b ({target.mean() * 100:.1f}%)  
‚îú‚îÄ‚îÄ üî¥ Nie u≈ºywa: {len(target) - target.sum()} os√≥b ({(1 - target.mean()) * 100:.1f}%)
‚îú‚îÄ‚îÄ üìö Zbi√≥r treningowy: {len(X_train)} obserwacji
‚îî‚îÄ‚îÄ üß™ Zbi√≥r testowy: {len(X_test)} obserwacji

üèÜ WYNIKI MODELU:
‚îú‚îÄ‚îÄ üéØ Dok≈Çadno≈õƒá (Accuracy): {accuracy:.3f} ({accuracy * 100:.1f}%)
‚îú‚îÄ‚îÄ üìà AUC-ROC: {auc_score:.3f}
‚îú‚îÄ‚îÄ üìä Baseline (najczƒôstsza klasa): {baseline:.3f} ({baseline * 100:.1f}%)
‚îî‚îÄ‚îÄ ‚¨ÜÔ∏è Poprawa nad baseline: {improvement:+.3f} ({improvement * 100:+.1f} pkt proc.)

üé≠ MACIERZ KONFUZJI:
                 Przewidywane
Rzeczywiste    Nie u≈ºywa  U≈ºywa
Nie u≈ºywa         {conf_matrix[0, 0]:>3}      {conf_matrix[0, 1]:>3}
U≈ºywa             {conf_matrix[1, 0]:>3}      {conf_matrix[1, 1]:>3}

üèÜ RANKING WA≈ªNO≈öCI CECH:
"""

        for _, row in feature_importance.iterrows():
            text += f"{row['Feature']:<18} {row['Importance']:.4f}\n"

        # Ocena wydajno≈õci
        status = get_performance_status(accuracy, baseline, auc_score)
        text += f"\nüìã OCENA WYDAJNO≈öCI: {status}\n"

        # Interpretacja
        category = get_substance_category(substance)
        text += f"\nüí° INTERPRETACJA:\n"
        text += f"‚Ä¢ Kategoria substancji: {category}\n"
        text += f"‚Ä¢ Przewidywalno≈õƒá: {'Wysoka' if improvement > 0.1 else 'Umiarkowana' if improvement > 0.05 else 'Niska'}\n"
        text += f"‚Ä¢ Najwa≈ºniejsza cecha: {feature_importance.iloc[0]['Feature']}\n"

        if improvement > 0.1:
            text += "‚úÖ Model dobrze przewiduje u≈ºywanie tej substancji na podstawie osobowo≈õci\n"
        elif improvement > 0.05:
            text += "‚ö†Ô∏è Model ma umiarkowanƒÖ zdolno≈õƒá przewidywania\n"
        else:
            text += "‚ùå Cechy osobowo≈õci s≈Çabo przewidujƒÖ u≈ºywanie tej substancji\n"

        return text

    def _generate_comparison_report(self, results_df: pd.DataFrame) -> str:
        """Generuje raport por√≥wnawczy wszystkich substancji"""

        text = "üèÜ === RANKING PRZEWIDYWALNO≈öCI WSZYSTKICH SUBSTANCJI ===\n\n"
        text += "ü§ñ Analiza ka≈ºdej substancji Random Forest...\n\n"

        text += "üèÜ RANKING PRZEWIDYWALNO≈öCI:\n"
        text += "=" * 80 + "\n"
        text += f"{'Rang':<4} {'Substancja':<15} {'U≈ºyt.':<6} {'%U≈ºyt':<6} {'Dok≈Ç.':<6} {'+Base':<6} {'AUC':<6} {'Status':<12} {'Top Cecha':<15}\n"
        text += "=" * 80 + "\n"

        for i, (_, row) in enumerate(results_df.iterrows(), 1):
            if pd.isna(row['Accuracy']):
                accuracy_str = "N/A"
                improvement_str = "N/A"
                auc_str = "N/A"
            else:
                accuracy_str = f"{row['Accuracy']:.3f}"
                improvement_str = f"{row['Improvement']:+.3f}"
                auc_str = f"{row['AUC']:.3f}"

            usage_pct = f"{row['Usage_Rate'] * 100:.1f}%"

            text += f"{i:<4} {row['Substance']:<15} {row['Users']:<6} {usage_pct:<6} {accuracy_str:<6} {improvement_str:<6} {auc_str:<6} {row['Status']:<12} {row['Top_Feature']:<15}\n"

        # Analiza wed≈Çug kategorii
        valid_results = results_df[~pd.isna(results_df['Accuracy'])]

        if len(valid_results) > 0:
            text += "\nüìä ANALIZA WED≈ÅUG KATEGORII:\n"
            text += "=" * 40 + "\n"

            categories = valid_results['Category'].unique()
            for category in categories:
                cat_data = valid_results[valid_results['Category'] == category]
                avg_improvement = cat_data['Improvement'].mean()
                avg_auc = cat_data['AUC'].mean()
                count = len(cat_data)

                cat_emoji = {
                    'Legal': 'üü¢',
                    'Soft Illegal': 'üü°',
                    'Stimulants': 'üü†',
                    'Hard Drugs': 'üî¥'
                }.get(category, '‚ùì')

                text += f"{cat_emoji} {category}: {count} substancji, ≈õrednie +{avg_improvement:.3f} improvement, AUC {avg_auc:.3f}\n"

            # Top predyktywne cechy
            feature_counts = valid_results['Top_Feature'].value_counts()
            text += f"\nüéØ NAJCZƒò≈öCIEJ NAJWA≈ªNIEJSZE CECHY:\n"
            text += "‚îÄ" * 35 + "\n"
            for feature, count in feature_counts.head(5).items():
                if feature != 'Insufficient data' and not feature.startswith('Error'):
                    text += f"üèÜ {feature}: {count} substancji ({count / len(valid_results) * 100:.0f}%)\n"

            # Podsumowanie wydajno≈õci
            excellent_count = len(valid_results[valid_results['Status'].str.contains('Excellent')])
            good_count = len(valid_results[valid_results['Status'].str.contains('Good')])

            text += f"\nüìà PODSUMOWANIE WYDAJNO≈öCI:\n"
            text += "‚îÄ" * 25 + "\n"
            text += f"üü¢ Excellent models: {excellent_count}\n"
            text += f"üü° Good models: {good_count}\n"
            text += f"üìä ≈örednia poprawa: +{valid_results['Improvement'].mean():.3f}\n"
            text += f"üìä ≈örednie AUC: {valid_results['AUC'].mean():.3f}\n"

        return text

    def _create_classification_plots(self, feature_importance: pd.DataFrame,
                                     accuracy: float, baseline: float,
                                     substance: str, auc_score: float) -> plt.Figure:
        """Tworzy wykresy dla klasyfikacji pojedynczej substancji"""

        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

        # Wykres wa≈ºno≈õci cech
        bars = ax1.barh(feature_importance['Feature'], feature_importance['Importance'],
                        color=COLORS['cluster_colors'], alpha=0.8)
        ax1.set_xlabel('Wa≈ºno≈õƒá cechy', fontweight='bold')
        ax1.set_title(f'üå≤ Wa≈ºno≈õƒá Cech - {substance}', fontweight='bold')
        ax1.grid(True, alpha=0.3, axis='x')

        # Por√≥wnanie wydajno≈õci
        categories = ['Baseline', 'Random Forest']
        scores = [baseline, accuracy]
        colors = [COLORS['danger'], COLORS['success']]

        bars2 = ax2.bar(categories, scores, color=colors, alpha=0.8)
        ax2.set_ylabel('Dok≈Çadno≈õƒá', fontweight='bold')
        ax2.set_title('üìä Por√≥wnanie Wydajno≈õci', fontweight='bold')
        ax2.set_ylim(0, 1)
        ax2.grid(True, alpha=0.3, axis='y')

        # Dodaj etykiety
        for bar, score in zip(bars2, scores):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width() / 2., height + 0.01,
                     f'{score:.1%}', ha='center', va='bottom', fontweight='bold')

        # Dodaj AUC score
        ax2.text(0.5, 0.5, f'AUC: {auc_score:.3f}', transform=ax2.transAxes,
                 ha='center', va='center', fontweight='bold', fontsize=12,
                 bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))

        plt.tight_layout()
        return fig

    def _create_comparison_plots(self, results_df: pd.DataFrame) -> Optional[plt.Figure]:
        """Tworzy wykresy por√≥wnawcze wszystkich substancji"""

        valid_results = results_df[~pd.isna(results_df['Accuracy'])]

        if len(valid_results) == 0:
            return None

        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(16, 12))

        # Scatter plot: Usage Rate vs Predictability
        category_colors = {
            'Legal': COLORS['success'],
            'Soft Illegal': COLORS['warning'],
            'Stimulants': COLORS['secondary'],
            'Hard Drugs': COLORS['danger']
        }

        for category in valid_results['Category'].unique():
            cat_data = valid_results[valid_results['Category'] == category]
            ax1.scatter(cat_data['Usage_Rate'] * 100, cat_data['AUC'] * 100,
                        c=category_colors.get(category, 'gray'),
                        label=category, alpha=0.7, s=100, edgecolors='white')

        ax1.set_xlabel('Odsetek u≈ºytkownik√≥w (%)', fontweight='bold')
        ax1.set_ylabel('AUC Score (%)', fontweight='bold')
        ax1.set_title('üéØ Przewidywalno≈õƒá vs Popularno≈õƒá Substancji', fontweight='bold')
        ax1.grid(True, alpha=0.3)
        ax1.legend()

        # Dodaj etykiety substancji
        for _, row in valid_results.iterrows():
            ax1.annotate(row['Substance'],
                         (row['Usage_Rate'] * 100, row['AUC'] * 100),
                         xytext=(5, 5), textcoords='offset points',
                         fontsize=8, alpha=0.7)

        # Czƒôsto≈õƒá najwa≈ºniejszych cech
        feature_counts = valid_results['Top_Feature'].value_counts()
        # Filtruj b≈Çƒôdy i brak danych
        feature_counts = feature_counts[
            ~feature_counts.index.str.contains('Error|Insufficient', na=False)
        ]

        if len(feature_counts) > 0:
            bars = ax2.barh(range(len(feature_counts)), feature_counts.values,
                            color=COLORS['cluster_colors'][:len(feature_counts)])
            ax2.set_yticks(range(len(feature_counts)))
            ax2.set_yticklabels(feature_counts.index)
            ax2.set_xlabel('Liczba substancji', fontweight='bold')
            ax2.set_title('üèÜ Najczƒô≈õciej Najwa≈ºniejsze Cechy Osobowo≈õci', fontweight='bold')
            ax2.grid(True, alpha=0.3, axis='x')

            # Dodaj etykiety warto≈õci
            for i, (bar, count) in enumerate(zip(bars, feature_counts.values)):
                width = bar.get_width()
                ax2.text(width + 0.1, bar.get_y() + bar.get_height() / 2,
                         f'{count}', ha='left', va='center', fontweight='bold')

        plt.tight_layout()
        return fig